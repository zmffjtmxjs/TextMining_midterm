---
title: "텍스트 마이닝 중간고사"
author: "20161555-BGH"
date: '2022 5 1 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#라이브러리 로드
library(dplyr)
library(readr)
library(stringr)
library(textclean)
library(tidytext)
library(KoNLP)
library(ggplot2)
```

### 문제1. 데이터 선정이유 구조 및 내용 설명
20대 대통령선거 출범식떄 윤석열과 이재명이 각자 연설한 내용입니다.

강의에서 실습에 사용하는 연설문 형식으로 같으나 제시된 문제 실습에 유효한 결과를 도출할 수 있을 것으로 판단하여
해당 데이터를 선정하였습니다.

###### 해당 분석은 실습을 목적으로 진행된 것이며 어떠한 정치적 의미를 가지지 않습니다.

### 문제2. 가장 자주 사용된 단어 추출 및 빈도 그래프 만들기
#### 코드 작성 및 분석 과정
##### 0) 데이터 전처리
```{r}
yun_speech =
  #파일 불러오기
  readLines("yun_speech.txt", encoding = "UTF-8") %>%
  #티블로 변환
  as_tibble() %>%
  #병합 시 구분을 위한 변수 추가
  mutate(president = "yun")

lee_speech =
  #파일 불러오기
  readLines("lee_speech.txt", encoding = "UTF-8") %>%
  #티블로 변환
  as_tibble() %>%
  #병합 시 구분을 위한 변수 추가
  mutate(president = "lee")

#두 개의 연설문을 하나로 병합
speeches = bind_rows(yun_speech, lee_speech) %>%
  select(president, value) %>% 
  #한글이 아닌 문자 전부 제거 ...
  mutate(value = str_replace_all(value, pattern = "[^가-힣]",  replacement = " "),
         #... 그리고 띄어쓰기가 2개이상 연속된 문자열을 하나로 압축
         value = str_squish(value))

#전처리 결과 확인
speeches
```

##### 1) 가장 자주 사용된 단어 추출
```{r}
#출범식 연설문 단어 추출
(speeches_freq = speeches %>%
  #전처리한 데이터를 형태소 분석기로 토큰화
  unnest_tokens(input = value, output = word, token = extractNoun) %>%
  #동일 단어 갯수를 세고 그 수를 기준으로 내림차순 정렬. 단, president 변수를 유지한다.
  count(president, word, sort = T) %>%
  #한 글자 이하인 모든 단어 제거
  filter(str_count(word) > 1))
```

##### 2) 빈도 그래프 만들기
```{r}
top10_president_keywrod = speeches_freq %>%
  group_by(president) %>%
  slice_max(n, n=10, with_ties = F)

ggplot(top10_president_keywrod,                 #그래프에 사용할 데이터 지정
       #x축은 reorder_within(축, 정렬기준, 나누는 기준)으로 임의 기준별 정렬
       aes(x = reorder_within(word, n, president),            
           #y축 기준 변수 지정
           y = n, fill = president)) +                        
  #막대 그래프를 그림
  geom_col() +                                                
  #가로 => 세로 변형
  coord_flip() +                                              
  #scales = 'free_y' 추가로 각 기준(president)별로 표 x축을 완전히 나눔
  facet_wrap(~president, scales = "free_y") +                 
  #그래프에 생성되는 불필요한(__moon과 같은) 항목이름 제거
  scale_x_reordered() +                                       
  #막대바 끝 숫자 위치 오프셋
  geom_text(aes(label = n), hjust = -0.3) + 
  #y축 최대값 설정
  ylim(0, 40) +
  #제목, X&Y축 제목
  labs(title = "연설문 단어 빈도", x = "단어", y = "횟수") +  
  #글자 크기 변경
  theme(title = element_text(size =10))
```

### 문제3. 오즈비 또는 TF-IDF 활용하여 분석하기
#### 코드 및 분석 과정
```{r}
#TF-IDF를 활용하여 연설문 분석
speeches_tf_idf = speeches_freq %>%
  bind_tf_idf(term = word, document = president, n = n) %>%
  group_by(president) %>%
  slice_max(tf_idf, n = 10, with_ties = F)
speeches_tf_idf$president = factor(speeches_tf_idf$president, levels = c("lee", "yun"))

#분석을 위해 TF-IDF 수치로 그래프 그리기
ggplot(speeches_tf_idf,                                 #그래프 사용 데이터 지정
       # x축은 tf_idf수치를 기준으로 word를 재정렬하고 president로 묶는다.
       aes(x = reorder_within(word, tf_idf, president),
           # y축은 tf_idf 수치로 데이터 막대를 그린다.
           y = tf_idf,
           # 대통령(president) 기준으로 색지정
           fill = president)) +
  #막대 그래프를 그리며 범례(legend)를 표시 하지 않음
  geom_col(show.legend = F) +
  #세로 막대 그래프로 변경
  coord_flip() +
  # president별로 막대그래프를 따로그린다.
  facet_wrap(~president,
             # x축과 y축을 각 막대그래프에 독립적으로 조절한다.
             scales = "free",
             # 2열로 배치한다.
             ncol = 2) +
  #x축에 ___yun과 같이 불필요한 어미를 제거한다.
  scale_x_reordered() + 
  #x축과 y축의 라벨을 제거한다.
  labs(x = NULL, y = NULL)
```
#### 결과 해석
TF-IDF는 흔하지 않고 한 텍스트에서 자주 사용되면 높은 값을 나타내는데 위의 결과에서 단어가 "가지", "이재" 같은 의미 없는 단어를 제외했을 때

이재명의 연설문에서는 "성장"을 강조하며 그 다음으로 "부동산, 민생, 회복"순서로 특징성을 보이며 그래프에 나온 단어들을 조합해보면 "성장을 위해 부동산과 민생 회복 관련으로 대전환(또는 개혁)을 시도할 것"으로 보인다.

윤석열의 연설문에서는 "정권교체"을 강조하며 그 다음으로 "생각, 내년, 가구"순으로 특징성을 보이며 그래프의 결과에서 "정권교체"의 TF-IDF 수치가 두 연설문의 단어 중 제일 높아 "정권교체를 하겠다"는 의지가 큼을 볼 수 있다.


### 문제4. 감정사전을 적용하여, 텍스트의 감정 경향을 분석하기

### 문제5. 감정사전 수정하여 적용하고, 수정전과 비교분석하기

### 추가점수문제. github로 버전관리 진행하고, 그 과정을 증비하기