---
title: "텍스트 마이닝 중간고사"
author: "20161555-BGH"
date: '2022 5 1 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#라이브러리 로드
library(dplyr)
library(readr)
library(stringr)
library(textclean)
library(tidytext)
library(KoNLP)
library(ggplot2)
```

#### 문제1. 데이터 선정이유 구조 및 내용 설명
20대 대통령선거 출범식떄 윤석열과 이재명이 각자 연설한 내용입니다.

강의에서 실습에 사용하는 연설문 형식으로 같으나 제시된 문제 실습에 유효한 결과를 도출할 수 있을 것으로 판단하여
해당 데이터를 선정하였습니다.

###### 해당 분석은 실습을 목적으로 진행된 것이며 어떠한 정치적 의미를 가지지 않습니다.

#### 문제2. 가장 자주 사용된 단어 추출 및 빈도 그래프 만들기
##### 0) 데이터 전처리
```{r}
yun_speech =
  #파일 불러오기
  readLines("yun_speech.txt", encoding = "UTF-8") %>%
  #티블로 변환
  as_tibble() %>%
  #병합 시 구분을 위한 변수 추가
  mutate(president = "yun")

lee_speech =
  #파일 불러오기
  readLines("lee_speech.txt", encoding = "UTF-8") %>%
  #티블로 변환
  as_tibble() %>%
  #병합 시 구분을 위한 변수 추가
  mutate(president = "lee")

#두 개의 연설문을 하나로 병합
speeches = bind_rows(yun_speech, lee_speech) %>%
  select(president, value) %>% 
  #한글이 아닌 문자 전부 제거 ...
  mutate(value = str_replace_all(value, pattern = "[^가-힣]",  replacement = " "),
         #... 그리고 띄어쓰기가 2개이상 연속된 문자열을 하나로 압축
         value = str_squish(value))

#전처리 결과 확인
speeches
```

##### 1) 가장 자주 사용된 단어 추출
```{r}
#출범식 연설문 단어 추출
(speeches_freq = speeches %>%
  #전처리한 데이터를 형태소 분석기로 토큰화
  unnest_tokens(input = value, output = word, token = extractNoun) %>%
  #동일 단어 갯수를 세고 그 수를 기준으로 내림차순 정렬. 단, president 변수를 유지한다.
  count(president, word, sort = T) %>%
  #한 글자 이하인 모든 단어 제거
  filter(str_count(word) > 1))
```

##### 2) 빈도 그래프 만들기
```{r}
top10_president_keywrod = speeches_freq %>%
  group_by(president) %>%
  slice_max(n, n=10, with_ties = F)

ggplot(top10_president_keywrod,                 #그래프에 사용할 데이터 지정
       #x축은 reorder_within(축, 정렬기준, 나누는 기준)으로 임의 기준별 정렬
       aes(x = reorder_within(word, n, president),            
           #y축 기준 변수 지정
           y = n, fill = president)) +                        
  #막대 그래프를 그림
  geom_col() +                                                
  #가로 => 세로 변형
  coord_flip() +                                              
  #scales = 'free_y' 추가로 각 기준(president)별로 표 x축을 완전히 나눔
  facet_wrap(~president, scales = "free_y") +                 
  #그래프에 생성되는 불필요한(__moon과 같은) 항목이름 제거
  scale_x_reordered() +                                       
  #막대바 끝 숫자 위치 오프셋
  geom_text(aes(label = n), hjust = -0.3) + 
  #y축 최대값 설정
  ylim(0, 40) +
  #제목, X&Y축 제목
  labs(title = "연설문 단어 빈도", x = "단어", y = "횟수") +  
  #글자 크기 변경
  theme(title = element_text(size =10))
```

#### 문제3. 오즈비 또는 TF-IDF 활용하여 분석하기
```{r}
#TF-IDF를 활용하여 연설문 분석
speeches_tf_idf = speeches_freq %>%
  bind_tf_idf(term = word, document = president, n = n) %>%
  arrange(-tf_idf)

#tf기준 내림차순으로 정렬
(arg_tf = speeches_tf_idf %>% arrange(-tf))
#tf_idf기준 내림차순으로 정렬
(arg_tf_idf = speeches_tf_idf %>% arrange(-tf_idf))
```

#### 문제4. 감정사전을 적용하여, 텍스트의 감정 경향을 분석하기

#### 문제5. 감정사전 수정하여 적용하고, 수정전과 비교분석하기

#### 추가점수문제. github로 버전관리 진행하고, 그 과정을 증비하기